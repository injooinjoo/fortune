import { serve } from 'https://deno.land/std@0.168.0/http/server.ts'
import { LLMFactory } from '../_shared/llm/factory.ts'
import { UsageLogger } from '../_shared/llm/usage-logger.ts'

/**
 * chat-insight-followup: ë¶„ì„ ê²°ê³¼ ê¸°ë°˜ í›„ì† ìƒë‹´ ë‹µë³€
 * ë¹„ë‚œ/ë‹¨ì • ê¸ˆì§€, ê°ì • ë°°ë ¤, ìœ„í—˜ ìƒí™© ì „ë¬¸ê°€ ì•ˆë‚´
 */

interface FollowupRequest {
  analysis_result: {
    followup_memory: {
      safe_notes: string
      user_questions: string[]
    }
    scores: Record<string, unknown>
    guidance: Record<string, unknown>
  }
  user_question: string
  conversation_history: Array<{ role: string; content: string }>
}

const SYSTEM_PROMPT = `ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ê´€ê³„ ê³ ë¯¼ì„ ë•ëŠ” "ëŒ€í™” ìƒë‹´ì‚¬"ì…ë‹ˆë‹¤. ì´ì „ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë”°ëœ»í•˜ê³  ì‹¤ìš©ì ì¸ ì¡°ì–¸ì„ ì œê³µí•©ë‹ˆë‹¤.

## ì•ˆì „ ê·œì¹™
- íŠ¹ì •ì¸ì„ ë¹„ë‚œí•˜ê±°ë‚˜ ë‹¨ì • ì§“ì§€ ë§ˆì„¸ìš”
- ì‚¬ìš©ìì˜ ê°ì •ì„ ë¨¼ì € ê³µê°í•œ í›„ ì¡°ì–¸
- "ì •ë‹µ"ì´ ì•„ë‹Œ "ì„ íƒì§€"ë¥¼ ì œì‹œ
- ìœ„í—˜ ìƒí™©(ìí•´/í­ë ¥/ìŠ¤í† í‚¹) ê°ì§€ ì‹œ ì „ë¬¸ê°€ ì•ˆë‚´ ìš°ì„  (ì •ì‹ ê±´ê°• ìœ„ê¸°ìƒë‹´ 1577-0199)

## í†¤ ê°€ì´ë“œ
- ì¹œêµ¬ê°™ì§€ë§Œ ì „ë¬¸ì ì¸ í†¤ (ë°˜ë§X, ì¡´ëŒ“ë§O)
- "~í•´ë³´ëŠ” ê±´ ì–´ë–¨ê¹Œìš”?", "~ì¼ ìˆ˜ ìˆì–´ìš”" í˜•íƒœ
- êµ¬ì²´ì  í–‰ë™ ì œì•ˆ í¬í•¨ (ì¶”ìƒì  ì¡°ì–¸ ê¸ˆì§€)
- í•œ ë²ˆì— 3-5ë¬¸ì¥ ì´ë‚´ë¡œ ë‹µë³€ (ê¸´ ì„¤êµ ê¸ˆì§€)

## ì¶œë ¥ í¬ë§·
plain text (í•œêµ­ì–´), 3-5ë¬¸ì¥. í•„ìš”ì‹œ "ğŸ’¡ íŒ:" ì ‘ë‘ì–´ë¡œ ì‹¤ìš©ì  í–‰ë™ ì œì•ˆ ì¶”ê°€.`

serve(async (req) => {
  if (req.method === 'OPTIONS') {
    return new Response('ok', {
      headers: {
        'Access-Control-Allow-Origin': '*',
        'Access-Control-Allow-Methods': 'POST',
        'Access-Control-Allow-Headers': 'authorization, x-client-info, apikey, content-type',
      },
    })
  }

  try {
    const requestData: FollowupRequest = await req.json()
    const { analysis_result, user_question, conversation_history = [] } = requestData

    if (!user_question || user_question.trim().length === 0) {
      throw new Error('ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.')
    }

    const llm = await LLMFactory.createFromConfigAsync('chat-insight')

    const contextPrompt = `## ì´ì „ ë¶„ì„ ìš”ì•½
${analysis_result.followup_memory?.safe_notes || 'ë¶„ì„ ê²°ê³¼ ì—†ìŒ'}

## ì£¼ìš” ì ìˆ˜
${JSON.stringify(analysis_result.scores || {})}

## í–‰ë™ ê°€ì´ë“œ
${JSON.stringify(analysis_result.guidance || {})}

## ì´ì „ ëŒ€í™”
${conversation_history.slice(-10).map(m => `${m.role}: ${m.content}`).join('\n')}

## ì‚¬ìš©ì ì§ˆë¬¸
${user_question}`

    const response = await llm.generate([
      { role: 'system', content: SYSTEM_PROMPT },
      { role: 'user', content: contextPrompt },
    ], {
      temperature: 0.8,
      maxTokens: 1024,
    })

    console.log(`âœ… chat-insight-followup ì™„ë£Œ: ${response.provider}/${response.model} - ${response.latency}ms`)

    await UsageLogger.log({
      fortuneType: 'chat-insight-followup',
      provider: response.provider,
      model: response.model,
      response: response,
      metadata: { question_length: user_question.length },
    })

    return new Response(JSON.stringify({
      success: true,
      data: {
        answer: response.content,
        timestamp: new Date().toISOString(),
      },
    }), {
      headers: {
        'Content-Type': 'application/json; charset=utf-8',
        'Access-Control-Allow-Origin': '*',
      },
    })
  } catch (error) {
    console.error('âŒ chat-insight-followup ì—ëŸ¬:', error)
    return new Response(JSON.stringify({
      success: false,
      error: error instanceof Error ? error.message : 'ìƒë‹´ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤',
    }), {
      status: 500,
      headers: {
        'Content-Type': 'application/json; charset=utf-8',
        'Access-Control-Allow-Origin': '*',
      },
    })
  }
})
