import 'dart:convert';
import 'dart:io';
import 'package:xml/xml.dart';
import '../data/models/celebrity_master_list.dart';
import '../data/models/celebrity.dart';

/// ë‚˜ë¬´ìœ„í‚¤ ë¤í”„ íŒŒì¼ì„ ì²˜ë¦¬í•˜ì—¬ ì—°ì˜ˆì¸ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” ì„œë¹„ìŠ¤
/// 
/// ì‚¬ìš© ë°©ë²•:
/// 1. ë‚˜ë¬´ìœ„í‚¤ ë¤í”„ ë‹¤ìš´ë¡œë“œ: https://dumps.namu.wiki/
/// 2. ë¤í”„ íŒŒì¼ì„ ì§€ì •ëœ ê²½ë¡œì— ì €ì¥
/// 3. ì´ ì„œë¹„ìŠ¤ë¡œ íŒŒì‹± ë° ë°ì´í„° ì¶”ì¶œ
class NamuWikiDumpProcessor {
  final String dumpFilePath;
  
  NamuWikiDumpProcessor({
    required this.dumpFilePath,
  });

  /// ë¤í”„ íŒŒì¼ì—ì„œ íŠ¹ì • ì—°ì˜ˆì¸ì˜ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤
  Future<CelebrityInfo?> extractCelebrityInfo(String celebrityName) async {
    try {
      debugPrint('ğŸ” ë¤í”„ì—ì„œ ê²€ìƒ‰ ì¤‘: $celebrityName');

      // XML ë¤í”„ íŒŒì¼ ì½ê¸° (ìŠ¤íŠ¸ë¦¼ ë°©ì‹ìœ¼ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ)
      final file = File(dumpFilePath);
      if (!await file.exists()) {
        throw Exception('ë¤í”„ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: $dumpFilePath');
      }

      final stream = file.openRead();
      String? pageContent;
      bool foundPage = false;

      await for (String chunk in stream.transform(utf8.decoder)) {
        // XML íŒŒì‹±í•˜ì—¬ í•´ë‹¹ í˜ì´ì§€ ì°¾ê¸°
        if (chunk.contains('<title>$celebrityName</title>')) {
          foundPage = true;
          // í•´ë‹¹ í˜ì´ì§€ì˜ ë‚´ìš© ì¶”ì¶œ
          pageContent = await _extractPageContent(celebrityName, stream);
          break;
        }
      }

      if (!foundPage || pageContent == null) {
        debugPrint('âŒ ë¤í”„ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŒ: $celebrityName');
        return null;
      }

      // ìœ„í‚¤í…ìŠ¤íŠ¸ì—ì„œ ì •ë³´ ì¶”ì¶œ
      final info = _parseWikiText(celebrityName, pageContent);
      debugPrint('âœ… ë¤í”„ì—ì„œ ì¶”ì¶œ ì™„ë£Œ: $celebrityName');
      
      return info;

    } catch (e) {
      debugPrint('âŒ ë¤í”„ ì²˜ë¦¬ ì˜¤ë¥˜ ($celebrityName): $e');
      return null;
    }
  }

  /// ì—¬ëŸ¬ ì—°ì˜ˆì¸ì˜ ì •ë³´ë¥¼ ë°°ì¹˜ë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤
  Future<Map<String, CelebrityInfo>> extractMultipleCelebrities(
    List<String> celebrityNames,
  ) async {
    debugPrint('ğŸ“‹ ë°°ì¹˜ ì¶”ì¶œ ì‹œì‘: ${celebrityNames.length}ëª…');

    final results = <String, CelebrityInfo>{};
    
    try {
      // ì „ì²´ ë¤í”„ íŒŒì¼ì„ í•œ ë²ˆë§Œ ì½ì–´ì„œ ëª¨ë“  ì—°ì˜ˆì¸ ì •ë³´ ì¶”ì¶œ
      final celebrityPages = await _extractMultiplePages(celebrityNames);

      for (final entry in celebrityPages.entries) {
        final name = entry.key;
        final content = entry.value;
        
        final info = _parseWikiText(name, content);
        if (info != null) {
          results[name] = info;
        }
      }

      debugPrint('ğŸ“Š ë°°ì¹˜ ì¶”ì¶œ ì™„ë£Œ: ${results.length}/${celebrityNames.length}ëª… ì„±ê³µ');
      
    } catch (e) {
      debugPrint('âŒ ë°°ì¹˜ ì¶”ì¶œ ì˜¤ë¥˜: $e');
    }

    return results;
  }

  /// ë¤í”„ íŒŒì¼ì—ì„œ ì—¬ëŸ¬ í˜ì´ì§€ë¥¼ í•œ ë²ˆì— ì¶”ì¶œ
  Future<Map<String, String>> _extractMultiplePages(List<String> names) async {
    final results = <String, String>{};
    final nameSet = names.toSet();
    
    final file = File(dumpFilePath);
    final stream = file.openRead();
    
    String buffer = '';
    String? currentTitle;
    bool inPage = false;
    bool inText = false;
    StringBuffer contentBuffer = StringBuffer();

    await for (String chunk in stream.transform(utf8.decoder).transform(LineSplitter())) {
      buffer = chunk;

      if (buffer.contains('<page>')) {
        inPage = true;
        currentTitle = null;
        contentBuffer.clear();
      } else if (buffer.contains('</page>')) {
        inPage = false;
        
        if (currentTitle != null && nameSet.contains(currentTitle)) {
          results[currentTitle] = contentBuffer.toString();
        }
        
        currentTitle = null;
        contentBuffer.clear();
      } else if (inPage && buffer.contains('<title>')) {
        final titleMatch = RegExp(r'<title>(.*?)</title>').firstMatch(buffer);
        if (titleMatch != null) {
          currentTitle = titleMatch.group(1);
        }
      } else if (inPage && buffer.contains('<text')) {
        inText = true;
        final textStart = buffer.indexOf('>');
        if (textStart != -1 && textStart < buffer.length - 1) {
          contentBuffer.write(buffer.substring(textStart + 1));
        }
      } else if (inText && buffer.contains('</text>')) {
        inText = false;
        final textEnd = buffer.indexOf('</text>');
        if (textEnd != -1) {
          contentBuffer.write(buffer.substring(0, textEnd));
        }
      } else if (inText) {
        contentBuffer.writeln(buffer);
      }

      // ëª¨ë“  ì—°ì˜ˆì¸ì„ ì°¾ì•˜ìœ¼ë©´ ì¡°ê¸° ì¢…ë£Œ
      if (results.length >= nameSet.length) {
        break;
      }
    }

    return results;
  }

  /// ë‹¨ì¼ í˜ì´ì§€ ë‚´ìš© ì¶”ì¶œ
  Future<String?> _extractPageContent(String name, Stream<List<int>> stream) async {
    // ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” XML ìŠ¤íŠ¸ë¦¼ íŒŒì‹±ì„ í†µí•´ í•´ë‹¹ í˜ì´ì§€ì˜ ë‚´ìš©ë§Œ ì¶”ì¶œ
    // ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ ë²„ì „ìœ¼ë¡œ êµ¬í˜„
    return null; // ì‹¤ì œë¡œëŠ” ìœ„í‚¤í…ìŠ¤íŠ¸ ë‚´ìš© ë°˜í™˜
  }

  /// ìœ„í‚¤í…ìŠ¤íŠ¸ì—ì„œ ì—°ì˜ˆì¸ ì •ë³´ë¥¼ íŒŒì‹±í•©ë‹ˆë‹¤
  CelebrityInfo? _parseWikiText(String name, String wikiText) {
    try {
      return CelebrityInfo(
        name: name,
        birthDate: _extractBirthDate(wikiText),
        birthTime: '12:00',
        gender: _extractGender(wikiText),
        category: _extractCategory(wikiText),
        description: _extractDescription(wikiText, name),
        profileImageUrl: _extractProfileImage(wikiText),
        keywords: _extractKeywords(wikiText, name),
        debut: _extractDebut(wikiText),
        agency: _extractAgency(wikiText),
        occupation: _extractOccupation(wikiText),
        aliases: _extractAliases(wikiText, name),
      );
    } catch (e) {
      debugPrint('ìœ„í‚¤í…ìŠ¤íŠ¸ íŒŒì‹± ì˜¤ë¥˜ ($name): $e');
      return null;
    }
  }

  /// ìœ„í‚¤í…ìŠ¤íŠ¸ì—ì„œ ìƒë…„ì›”ì¼ ì¶”ì¶œ
  String? _extractBirthDate(String wikiText) {
    final patterns = [
      RegExp(r'\|\s*ìƒë…„ì›”ì¼\s*=\s*(\d{4})ë…„?\s*(\d{1,2})ì›”?\s*(\d{1,2})ì¼?'),
      RegExp(r'\|\s*ì¶œìƒì¼\s*=\s*(\d{4})ë…„?\s*(\d{1,2})ì›”?\s*(\d{1,2})ì¼?'),
      RegExp(r'(\d{4})ë…„\s*(\d{1,2})ì›”\s*(\d{1,2})ì¼\s*ì¶œìƒ'),
      RegExp(r'(\d{4})\.\s*(\d{1,2})\.\s*(\d{1,2})\s*ì¶œìƒ'),
    ];

    for (final pattern in patterns) {
      final match = pattern.firstMatch(wikiText);
      if (match != null && match.groupCount >= 3) {
        final year = match.group(1);
        final month = match.group(2)?.padLeft(2, '0');
        final day = match.group(3)?.padLeft(2, '0');
        
        if (year != null && month != null && day != null) {
          final yearInt = int.tryParse(year);
          if (yearInt != null && yearInt >= 1900 && yearInt <= 2010) {
            return '$year-$month-$day';
          }
        }
      }
    }
    
    return null;
  }

  /// ìœ„í‚¤í…ìŠ¤íŠ¸ì—ì„œ ì„±ë³„ ì¶”ì¶œ
  String _extractGender(String wikiText) {
    if (wikiText.contains('ì—¬ì„±') || wikiText.contains('ì—¬ë°°ìš°') || wikiText.contains('ì—¬ê°€ìˆ˜') ||
        wikiText.contains('ê±¸ê·¸ë£¹') || wikiText.contains('ê·¸ë…€ëŠ”')) {
      return 'female';
    }
    return 'male';
  }

  /// ìœ„í‚¤í…ìŠ¤íŠ¸ì—ì„œ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
  String _extractCategory(String wikiText) {
    if (wikiText.contains('ë°°ìš°') || wikiText.contains('ì—°ê¸°ì')) return 'actor';
    if (wikiText.contains('ê°€ìˆ˜') || wikiText.contains('ìŒì•…ê°€') || wikiText.contains('ë³´ì»¬')) return 'singer';
    if (wikiText.contains('ì •ì¹˜ì¸') || wikiText.contains('êµ­íšŒì˜ì›') || wikiText.contains('ëŒ€í†µë ¹')) return 'politician';
    if (wikiText.contains('ìš´ë™ì„ ìˆ˜') || wikiText.contains('ì¶•êµ¬ì„ ìˆ˜') || wikiText.contains('ì•¼êµ¬ì„ ìˆ˜')) return 'sports';
    if (wikiText.contains('ìŠ¤íŠ¸ë¦¬ë¨¸') || wikiText.contains('BJ')) return 'streamer';
    if (wikiText.contains('ìœ íŠœë²„') || wikiText.contains('í¬ë¦¬ì—ì´í„°')) return 'youtuber';
    if (wikiText.contains('ê°œê·¸ë§¨') || wikiText.contains('ì½”ë¯¸ë””ì–¸')) return 'entertainer';
    if (wikiText.contains('í”„ë¡œê²Œì´ë¨¸')) return 'pro_gamer';
    if (wikiText.contains('ê¸°ì—…ì¸') || wikiText.contains('CEO') || wikiText.contains('íšŒì¥')) return 'business_leader';
    
    return 'entertainer';
  }

  /// ìœ„í‚¤í…ìŠ¤íŠ¸ì—ì„œ ì„¤ëª… ì¶”ì¶œ
  String _extractDescription(String wikiText, String name) {
    // ì²« ë²ˆì§¸ ë¬¸ë‹¨ì´ë‚˜ ê°œìš” ë¶€ë¶„ì—ì„œ ì„¤ëª… ì¶”ì¶œ
    final lines = wikiText.split('\n');
    for (final line in lines) {
      if (line.trim().isNotEmpty && 
          !line.startsWith('|') && 
          !line.startsWith('{{') && 
          !line.startsWith('[[') &&
          line.length > 20) {
        return line.trim().length > 200 ? '${line.trim().substring(0, 200)}...' : line.trim();
      }
    }
    return '$nameì— ëŒ€í•œ ì •ë³´';
  }

  /// í”„ë¡œí•„ ì´ë¯¸ì§€ ì¶”ì¶œ
  String? _extractProfileImage(String wikiText) {
    final imagePatterns = [
      RegExp(r'\|\s*ì‚¬ì§„\s*=\s*([^\|\n]+)'),
      RegExp(r'\|\s*ì´ë¯¸ì§€\s*=\s*([^\|\n]+)'),
      RegExp(r'\[\[íŒŒì¼:([^\]]+)\]\]'),
    ];

    for (final pattern in imagePatterns) {
      final match = pattern.firstMatch(wikiText);
      if (match != null && match.group(1) != null) {
        final imageFile = match.group(1)!.trim();
        if (imageFile.isNotEmpty) {
          // ë‚˜ë¬´ìœ„í‚¤ ì´ë¯¸ì§€ URL ìƒì„± (ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ë³€í™˜ì´ í•„ìš”)
          return 'https://w.namu.la/s/${Uri.encodeComponent(imageFile)}';
        }
      }
    }
    
    return null;
  }

  /// í‚¤ì›Œë“œ ì¶”ì¶œ
  List<String> _extractKeywords(String wikiText, String name) {
    final keywords = <String>{name};
    
    // ìì£¼ ë‚˜ì˜¤ëŠ” í‚¤ì›Œë“œë“¤ ì¶”ì¶œ
    final commonTerms = ['ë°ë·”', 'í™œë™', 'ì•¨ë²”', 'ë“œë¼ë§ˆ', 'ì˜í™”', 'ì¶œì—°', 'ì†Œì†', 'ê·¸ë£¹'];
    for (final term in commonTerms) {
      if (wikiText.contains(term)) {
        keywords.add(term);
      }
    }
    
    return keywords.toList();
  }

  /// ë°ë·” ì •ë³´ ì¶”ì¶œ
  String? _extractDebut(String wikiText) {
    final patterns = [
      RegExp(r'\|\s*ë°ë·”\s*=\s*([^\|\n]+)'),
      RegExp(r'\|\s*ë°ë·”ì‘\s*=\s*([^\|\n]+)'),
      RegExp(r'(\d{4}ë…„)\s*ë°ë·”'),
    ];

    for (final pattern in patterns) {
      final match = pattern.firstMatch(wikiText);
      if (match != null && match.group(1) != null) {
        return match.group(1)!.trim();
      }
    }
    
    return null;
  }

  /// ì†Œì†ì‚¬ ì¶”ì¶œ
  String? _extractAgency(String wikiText) {
    final patterns = [
      RegExp(r'\|\s*ì†Œì†ì‚¬\s*=\s*([^\|\n]+)'),
      RegExp(r'\|\s*ì†Œì†\s*=\s*([^\|\n]+)'),
      RegExp(r'\|\s*ë ˆì´ë¸”\s*=\s*([^\|\n]+)'),
    ];

    for (final pattern in patterns) {
      final match = pattern.firstMatch(wikiText);
      if (match != null && match.group(1) != null) {
        final agency = match.group(1)!.trim();
        if (agency.isNotEmpty && agency.length < 50) {
          return agency;
        }
      }
    }
    
    return null;
  }

  /// ì§ì—… ì¶”ì¶œ
  String? _extractOccupation(String wikiText) {
    final patterns = [
      RegExp(r'\|\s*ì§ì—…\s*=\s*([^\|\n]+)'),
      RegExp(r'\|\s*í™œë™ë¶„ì•¼\s*=\s*([^\|\n]+)'),
    ];

    for (final pattern in patterns) {
      final match = pattern.firstMatch(wikiText);
      if (match != null && match.group(1) != null) {
        return match.group(1)!.trim();
      }
    }
    
    return null;
  }

  /// ë³„ëª…/ì˜ˆëª… ì¶”ì¶œ
  List<String> _extractAliases(String wikiText, String name) {
    final aliases = <String>[];
    
    final patterns = [
      RegExp(r'\|\s*ë³„ëª…\s*=\s*([^\|\n]+)'),
      RegExp(r'\|\s*ì˜ˆëª…\s*=\s*([^\|\n]+)'),
      RegExp(r'\|\s*ë³¸ëª…\s*=\s*([^\|\n]+)'),
    ];

    for (final pattern in patterns) {
      final match = pattern.firstMatch(wikiText);
      if (match != null && match.group(1) != null) {
        final alias = match.group(1)!.trim();
        if (alias.isNotEmpty && alias != name && alias.length < 20) {
          aliases.add(alias);
        }
      }
    }
    
    return aliases;
  }

  /// ë¤í”„ íŒŒì¼ ì •ë³´ í™•ì¸
  Future<DumpFileInfo> getDumpFileInfo() async {
    final file = File(dumpFilePath);
    
    if (!await file.exists()) {
      throw Exception('ë¤í”„ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: $dumpFilePath');
    }

    final stat = await file.stat();
    
    return DumpFileInfo(
      filePath: dumpFilePath,
      fileSize: stat.size,
      lastModified: stat.modified,
      exists: true,
    );
  }
}

/// ì—°ì˜ˆì¸ ì •ë³´ ë°ì´í„° í´ë˜ìŠ¤
class CelebrityInfo {
  final String name;
  final String? birthDate;
  final String? birthTime;
  final String gender;
  final String category;
  final String description;
  final String? profileImageUrl;
  final List<String> keywords;
  final String? debut;
  final String? agency;
  final String? occupation;
  final List<String> aliases;

  CelebrityInfo({
    required this.name,
    this.birthDate,
    this.birthTime,
    required this.gender,
    required this.category,
    required this.description,
    this.profileImageUrl,
    required this.keywords,
    this.debut,
    this.agency,
    this.occupation,
    required this.aliases,
  });

  Map<String, dynamic> toJson() => {
    'name': name,
    'birth_date': birthDate,
    'birth_time': birthTime,
    'gender': gender,
    'category': category,
    'description': description,
    'profile_image_url': profileImageUrl,
    'keywords': keywords,
    'debut': debut,
    'agency': agency,
    'occupation': occupation,
    'aliases': aliases,
  };
}

/// ë¤í”„ íŒŒì¼ ì •ë³´ í´ë˜ìŠ¤
class DumpFileInfo {
  final String filePath;
  final int fileSize;
  final DateTime lastModified;
  final bool exists;

  DumpFileInfo({
    required this.filePath,
    required this.fileSize,
    required this.lastModified,
    required this.exists,
  });

  String get fileSizeFormatted => '${(fileSize / (1024 * 1024)).toStringAsFixed(1)} MB';
}